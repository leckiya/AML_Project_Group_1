{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3.9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c135b0e-b231-4ba3-9bf5-109004c988bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/leckiya/AML_Project/venv/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error, r2_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import R2Score, MeanIoU\n",
    "import itertools\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
    "import time\n",
    "import sys\n",
    "import psutil\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "OUTPUT_DIR = 'results_v6'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Global Constants ---\n",
    "DATASET_DIR = 'nutrition5k_dataset'\n",
    "METADATA_DIR = os.path.join(DATASET_DIR, 'metadata')\n",
    "IMAGERY_DIR = os.path.join(DATASET_DIR, 'imagery', 'realsense_overhead')\n",
    "SPLIT_DIR = os.path.join(DATASET_DIR, 'dish_ids', 'splits')\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- Custom Loss Function ---\n",
    "def normalized_metrics_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function that combines normalized MAE, RMSE, and R2 across 5 nutritional categories.\n",
    "    The loss is designed to be minimized, so we use (1 - R2) for R2 component.\n",
    "    \"\"\"\n",
    "    # Calculate metrics for each nutrient (5 categories)\n",
    "    mae_losses = []\n",
    "    rmse_losses = []\n",
    "    r2_losses = []\n",
    "    \n",
    "    for i in range(5):  # 5 nutrients: calories, mass, fat, carb, protein\n",
    "        y_true_nutrient = y_true[:, i]\n",
    "        y_pred_nutrient = y_pred[:, i]\n",
    "        \n",
    "        # MAE (normalized by mean of true values)\n",
    "        mae = tf.reduce_mean(tf.abs(y_true_nutrient - y_pred_nutrient))\n",
    "        mae_normalized = mae / (tf.reduce_mean(tf.abs(y_true_nutrient)) + 1e-8)\n",
    "        mae_losses.append(mae_normalized)\n",
    "        \n",
    "        # RMSE (normalized by mean of true values)\n",
    "        mse = tf.reduce_mean(tf.square(y_true_nutrient - y_pred_nutrient))\n",
    "        rmse = tf.sqrt(mse + 1e-8)\n",
    "        rmse_normalized = rmse / (tf.reduce_mean(tf.abs(y_true_nutrient)) + 1e-8)\n",
    "        rmse_losses.append(rmse_normalized)\n",
    "        \n",
    "        # R2 component (1 - R2, since we want to minimize loss)\n",
    "        # R2 = 1 - (SS_res / SS_tot)\n",
    "        ss_res = tf.reduce_sum(tf.square(y_true_nutrient - y_pred_nutrient))\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true_nutrient - tf.reduce_mean(y_true_nutrient)))\n",
    "        r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "        r2_loss = 1 - r2  # Convert to loss (minimize 1-R2)\n",
    "        r2_losses.append(r2_loss)\n",
    "    \n",
    "    # Average across all nutrients\n",
    "    avg_mae = tf.reduce_mean(mae_losses)\n",
    "    avg_rmse = tf.reduce_mean(rmse_losses)\n",
    "    avg_r2_loss = tf.reduce_mean(r2_losses)\n",
    "    \n",
    "    # Combine all metrics (equal weights)\n",
    "    total_loss = avg_mae + avg_rmse + avg_r2_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# --- 1. Data Loading and Preprocessing ---\n",
    "# Only RGB images are used: imagery/realsense_overhead/<dish_id>/rgb.png\n",
    "# Both cafe1 and cafe2 metadata are always loaded for the full dataset\n",
    "\n",
    "def load_full_data():\n",
    "    \"\"\"\n",
    "    Loads the entire Nutrition5k dataset, using both cafe1 and cafe2 metadata, and only RGB images.\n",
    "    Returns a DataFrame with all available samples and their metadata.\n",
    "    \"\"\"\n",
    "    print(f\"--- Loading FULL Nutrition5k dataset (ignoring split files) ---\")\n",
    "    # 1. Load all metadata\n",
    "    metadata_files = [os.path.join(METADATA_DIR, f) for f in ['dish_metadata_cafe1.csv', 'dish_metadata_cafe2.csv']]\n",
    "    df_list = []\n",
    "    col_names = ['dish_id', 'total_calories', 'total_mass', 'total_fat', 'total_carb', 'total_protein', 'num_ingrs']\n",
    "    for f_path in metadata_files:\n",
    "        try:\n",
    "            df = pd.read_csv(f_path, header=None, usecols=range(7), names=col_names, on_bad_lines='skip', engine='python')\n",
    "            df_list.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Metadata file not found at {f_path}\")\n",
    "            continue\n",
    "    if not df_list:\n",
    "        print(\"Error: No metadata files found. Aborting.\")\n",
    "        return pd.DataFrame()\n",
    "    metadata = pd.concat(df_list, ignore_index=True)\n",
    "    # 2. Construct file paths and verify existence\n",
    "    data = []\n",
    "    for _, row in metadata.iterrows():\n",
    "        dish_id = row['dish_id']\n",
    "        img_path = os.path.join(IMAGERY_DIR, dish_id, 'rgb.png')\n",
    "        if os.path.exists(img_path):\n",
    "            data.append({\n",
    "                'file_path': img_path,\n",
    "                'dish_id': dish_id,\n",
    "                'calories': row['total_calories'],\n",
    "                'mass': row['total_mass'],\n",
    "                'fat': row['total_fat'],\n",
    "                'carb': row['total_carb'],\n",
    "                'protein': row['total_protein']\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Found {len(df)} images in the full dataset.\")\n",
    "    if df.empty:\n",
    "        print(f\"Warning: No data loaded. Check paths and file existence.\")\n",
    "    return df\n",
    "\n",
    "def explore_and_process_data(df, split_name, caps=None):\n",
    "    \"\"\"\n",
    "    Performs data exploration and preprocessing.\n",
    "    For the test set, it uses the caps calculated from the training set.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Exploring and Processing {split_name} data ---\")\n",
    "    \n",
    "    nutritional_cols = ['calories', 'mass', 'fat', 'carb', 'protein']\n",
    "    \n",
    "    # 1. Descriptive Statistics\n",
    "    print(\"Descriptive Statistics for Nutritional Information (before processing):\")\n",
    "    print(df[nutritional_cols].describe())\n",
    "    \n",
    "    # 2. Check for missing values\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df[nutritional_cols].isnull().sum())\n",
    "    \n",
    "    # 3. Visualize distributions\n",
    "    print(\"\\nGenerating distribution plots...\")\n",
    "    for col in nutritional_cols:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.histplot(df[col], kde=True, bins=50)\n",
    "        plt.title(f'Distribution of {col} in {split_name} data (Original)')\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'{split_name}_{col}_distribution_original.png'))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. Outlier Capping\n",
    "    if split_name == 'train':\n",
    "        print(\"\\nCalculating caps and capping outliers at the 99th percentile for training data...\")\n",
    "        caps = {}\n",
    "        for col in nutritional_cols:\n",
    "            percentile_99 = df[col].quantile(0.99)\n",
    "            caps[col] = percentile_99\n",
    "            df[col] = df[col].apply(lambda x: min(x, percentile_99))\n",
    "            print(f\"Capped '{col}' at: {percentile_99:.2f}\")\n",
    "\n",
    "        print(\"\\nDescriptive Statistics after Outlier Capping:\")\n",
    "        print(df[nutritional_cols].describe())\n",
    "        return df, caps\n",
    "    else: # For 'test' or 'validation' data\n",
    "        if caps:\n",
    "            print(\"\\nCapping outliers in test data based on training set caps...\")\n",
    "            for col in nutritional_cols:\n",
    "                df[col] = df[col].apply(lambda x: min(x, caps[col]))\n",
    "            print(\"\\nDescriptive Statistics after Outlier Capping:\")\n",
    "            print(df[nutritional_cols].describe())\n",
    "        else:\n",
    "            print(\"\\nWarning: No caps provided for test data processing. Outliers not handled.\")\n",
    "        return df, None\n",
    "\n",
    "# --- 2. Model Building ---\n",
    "# ResNet50 is used as a feature extractor. It is a deep convolutional neural network with residual connections, allowing for very deep architectures without vanishing gradients. The model computes features by applying a series of convolutional, batch normalization, and ReLU layers, with skip connections that add the input of a block to its output. The final feature vector is obtained by global average pooling over the last convolutional feature maps.\n",
    "\n",
    "def build_regression_model():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), pooling='avg')\n",
    "    base_model.trainable = False # We will only use it for feature extraction\n",
    "    return base_model\n",
    "\n",
    "def build_regression_head(input_shape):\n",
    "    \"\"\"Builds the small regression model that trains on extracted features.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(5, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss=normalized_metrics_loss, metrics=[R2Score()])\n",
    "    return model\n",
    "\n",
    "def build_baseline_regression_head(input_shape):\n",
    "    # Baseline: minimal head, no regularization, high learning rate, few epochs\n",
    "    model = tf.keras.Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(5, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-2), loss=normalized_metrics_loss, metrics=[R2Score()])\n",
    "    return model\n",
    "\n",
    "# --- 3. Task Execution ---\n",
    "\n",
    "def extract_features(df, model):\n",
    "    \"\"\"\n",
    "    Uses the base ResNet50 model to extract features from all images.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Extracting Features ---\")\n",
    "    \n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    \n",
    "    generator = datagen.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        x_col='file_path',\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None, # We only want the images\n",
    "        shuffle=False # Keep order for matching with labels\n",
    "    )\n",
    "    \n",
    "    features = model.predict(generator, steps=len(generator), verbose=1)\n",
    "    print(f\"Extracted features with shape: {features.shape}\")\n",
    "    return features\n",
    "\n",
    "def run_regression_task(train_features, train_labels, test_features, test_labels, scaler, model_type='improved', epochs=200):\n",
    "    # model_type: 'baseline' or 'improved'\n",
    "    # epochs: number of epochs to train\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_features, train_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    if model_type == 'baseline':\n",
    "        model = build_baseline_regression_head(input_shape=(train_features.shape[1],))\n",
    "    else:\n",
    "        model = build_regression_head(input_shape=(train_features.shape[1],))\n",
    "    early_stopping = EarlyStopping(monitor='val_r2_score', patience=40, restore_best_weights=True, verbose=1, mode='max')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_r2_score', factor=0.2, patience=20, min_lr=1e-7, verbose=1, mode='max')\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping, reduce_lr] if model_type=='improved' else []\n",
    "    )\n",
    "    predictions_scaled = model.predict(test_features)\n",
    "    y_true_scaled = test_labels\n",
    "    predictions = scaler.inverse_transform(predictions_scaled)\n",
    "    y_true = scaler.inverse_transform(y_true_scaled)\n",
    "    # Per-nutrient metrics\n",
    "    metrics = {}\n",
    "    nutrients = ['Calories', 'Mass', 'Fat', 'Carb', 'Protein']\n",
    "    for i, nutrient in enumerate(nutrients):\n",
    "        mse = mean_squared_error(y_true[:, i], predictions[:, i])\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true[:, i], predictions[:, i])\n",
    "        r2 = r2_score(y_true[:, i], predictions[:, i])\n",
    "        mape = np.mean(np.abs((y_true[:, i] - predictions[:, i]) / (y_true[:, i] + 1e-8))) * 100\n",
    "        value_range = y_true[:, i].max() - y_true[:, i].min()\n",
    "        nrmse = rmse / (value_range + 1e-8)\n",
    "        nmae = mae / (value_range + 1e-8)\n",
    "        norm_factor = np.mean(np.abs(y_true[:, i]))\n",
    "        metrics[nutrient] = {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2,\n",
    "            'MAPE': mape,\n",
    "            'NRMSE': nrmse,\n",
    "            'NMAE': nmae,\n",
    "            'Norm factor': norm_factor\n",
    "        }\n",
    "    # Overall metrics (mean across all nutrients)\n",
    "    mse = mean_squared_error(y_true, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, predictions)\n",
    "    r2 = r2_score(y_true, predictions)\n",
    "    mape = np.mean(np.abs((y_true - predictions) / (y_true + 1e-8))) * 100\n",
    "    value_range = y_true.max() - y_true.min()\n",
    "    nrmse = rmse / (value_range + 1e-8)\n",
    "    nmae = mae / (value_range + 1e-8)\n",
    "    norm_factor = np.mean(np.abs(y_true))\n",
    "    metrics['Overall'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "        'NRMSE': nrmse,\n",
    "        'NMAE': nmae,\n",
    "        'Norm factor': norm_factor\n",
    "    }\n",
    "    return model, metrics, predictions, y_true, history\n",
    "\n",
    "def plot_training_history(history, model_type, output_dir):\n",
    "    \"\"\"\n",
    "    Plot training history for the custom loss function.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_type} Model - Custom Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot R2 Score\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['r2_score'], label='Training R2')\n",
    "    plt.plot(history.history['val_r2_score'], label='Validation R2')\n",
    "    plt.title(f'{model_type} Model - R2 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('R2 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot learning rate (if available)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    if 'lr' in history.history:\n",
    "        plt.plot(history.history['lr'], label='Learning Rate')\n",
    "        plt.title(f'{model_type} Model - Learning Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Learning Rate\\nNot Available', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title(f'{model_type} Model - Learning Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_type.lower()}_training_history.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def generate_summary_report(regression_history):\n",
    "    \"\"\"\n",
    "    Generates a summary report of all tasks.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Summary Report ---\")\n",
    "    \n",
    "    report_path = os.path.join(OUTPUT_DIR, 'summary_report.txt')\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"======================================\\n\")\n",
    "        print(\"======================================\")\n",
    "        f.write(\"      NUTRITION ANALYSIS REPORT      \\n\")\n",
    "        print(\"      NUTRITION ANALYSIS REPORT      \")\n",
    "        f.write(\"======================================\\n\\n\")\n",
    "        print(\"======================================\\n\")\n",
    "        # --- Regression Summary ---\n",
    "        f.write(\"--- Calorie/Nutrient Estimation Task ---\\n\")\n",
    "        print(\"--- Calorie/Nutrient Estimation Task ---\")\n",
    "        f.write(\"--- Using Custom Normalized Metrics Loss ---\\n\")\n",
    "        print(\"--- Using Custom Normalized Metrics Loss ---\")\n",
    "        if regression_history and regression_history.history:\n",
    "            best_epoch = np.argmin(regression_history.history['val_loss']) + 1\n",
    "            f.write(f\"Best model saved from epoch: {best_epoch}\\n\")\n",
    "            print(f\"Best model saved from epoch: {best_epoch}\")\n",
    "            f.write(f\"Final custom loss: {regression_history.history['loss'][-1]:.6f}\\n\")\n",
    "            print(f\"Final custom loss: {regression_history.history['loss'][-1]:.6f}\")\n",
    "            f.write(f\"Final validation custom loss: {regression_history.history['val_loss'][-1]:.6f}\\n\")\n",
    "            print(f\"Final validation custom loss: {regression_history.history['val_loss'][-1]:.6f}\")\n",
    "            reg_report_path = os.path.join(OUTPUT_DIR, 'regression_report.csv')\n",
    "            if os.path.exists(reg_report_path):\n",
    "                report_df = pd.read_csv(reg_report_path)\n",
    "                overall_metrics = report_df[report_df['Target'] == 'Overall']\n",
    "                calories_metrics = report_df[report_df['Target'] == 'Calories']\n",
    "                if not overall_metrics.empty and not calories_metrics.empty:\n",
    "                    r2_val = overall_metrics['R-squared (R2)'].values[0]\n",
    "                    r2 = float(r2_val) if r2_val != '-' else -1.0\n",
    "                    mae = calories_metrics['Mean Absolute Error (MAE)'].values[0]\n",
    "                    rmse = calories_metrics['Root Mean Squared Error (RMSE)'].values[0]\n",
    "                    f.write(f\"  - R-squared (Overall): {r2:.4f}\\n\")\n",
    "                    print(f\"  - R-squared (Overall): {r2:.4f}\")\n",
    "                    f.write(f\"  - Calories MAE: {mae:.4f}\\n\")\n",
    "                    print(f\"  - Calories MAE: {mae:.4f}\")\n",
    "                    f.write(f\"  - Calories RMSE: {rmse:.4f}\\n\")\n",
    "                    print(f\"  - Calories RMSE: {rmse:.4f}\")\n",
    "            else:\n",
    "                f.write(\"  - Regression report file not found.\\n\")\n",
    "                print(\"  - Regression report file not found.\")\n",
    "        else:\n",
    "            f.write(\"  - Regression task was skipped or failed.\\n\")\n",
    "            print(\"  - Regression task was skipped or failed.\")\n",
    "        f.write(\"\\n\")\n",
    "        print()\n",
    "\n",
    "    print(f\"Summary report saved to {report_path}\")\n",
    "\n",
    "def generate_final_report(train_df, test_df, regression_metrics):\n",
    "    report_path = os.path.join(OUTPUT_DIR, 'final_report.txt')\n",
    "    with open(report_path, 'w') as f:\n",
    "        # 1. Executive Summary\n",
    "        f.write(\"1. Executive Summary\\n\")\n",
    "        print(\"1. Executive Summary\")\n",
    "        f.write(\"====================\\n\")\n",
    "        print(\"====================\")\n",
    "        f.write(\"This project aims to automate food calorie estimation from images using a deep learning pipeline for calorie regression. The approach achieves accurate calorie prediction, with all results, metrics, and visualizations saved for reproducibility.\\n\\n\")\n",
    "        print(\"This project aims to automate food calorie estimation from images using a deep learning pipeline for calorie regression. The approach achieves accurate calorie prediction, with all results, metrics, and visualizations saved for reproducibility.\\n\")\n",
    "        # 2. Introduction\n",
    "        f.write(\"2. Introduction\\n\")\n",
    "        print(\"2. Introduction\")\n",
    "        f.write(\"===============\\n\")\n",
    "        print(\"===============\" )\n",
    "        f.write(\"Motivation: Health awareness and the challenge of accurate calorie tracking.\\n\")\n",
    "        print(\"Motivation: Health awareness and the challenge of accurate calorie tracking.\")\n",
    "        f.write(\"Problem: Automate calorie estimation from food images.\\n\")\n",
    "        print(\"Problem: Automate calorie estimation from food images.\")\n",
    "        f.write(\"Objectives: Build a robust, extensible pipeline for food calorie estimation.\\n\")\n",
    "        print(\"Objectives: Build a robust, extensible pipeline for food calorie estimation.\")\n",
    "        f.write(\"Business Applicability: Enables dietary feedback, mobile health, and future extensions.\\n\\n\")\n",
    "        print(\"Business Applicability: Enables dietary feedback, mobile health, and future extensions.\\n\")\n",
    "        # 3. Related Work / Literature Review\n",
    "        f.write(\"3. Related Work / Literature Review\\n\")\n",
    "        f.write(\"===================================\\n\")\n",
    "        f.write(\"- Existing tools: MyFitnessPal, CalorieMama.\\n\")\n",
    "        f.write(\"- Academic/industry research: Deep learning for food calorie estimation.\\n\")\n",
    "        f.write(\"- Gaps: Manual entry, limited generalization, lack of end-to-end automation.\\n\")\n",
    "        f.write(\"- Our originality: Robust metrics, extensible design.\\n\\n\")\n",
    "        print(\"- Existing tools: MyFitnessPal, CalorieMama.\")\n",
    "        print(\"- Academic/industry research: Deep learning for food calorie estimation.\")\n",
    "        print(\"- Gaps: Manual entry, limited generalization, lack of end-to-end automation.\")\n",
    "        print(\"- Our originality: Robust metrics, extensible design.\")\n",
    "        f.write(\"\\n\")\n",
    "        # 4. Dataset Description\n",
    "        f.write(\"4. Dataset Description\\n\")\n",
    "        f.write(\"======================\\n\")\n",
    "        f.write(f\"Origin: Nutrition5k Dataset (see README).\\n\")\n",
    "        f.write(f\"Train images: {len(train_df)}, Test images: {len(test_df)}\\n\")\n",
    "        f.write(f\"Categories: {train_df['dish_id'].nunique()}\\n\")\n",
    "        f.write(\"Structure: imagery/metadata/splits.\\n\")\n",
    "        f.write(\"Preprocessing: resizing to 224x224, normalization, outlier capping, augmentation.\\n\")\n",
    "        f.write(\"Biases: Some classes have few samples; test set may contain unseen classes.\\n\\n\")\n",
    "        print(f\"Origin: Nutrition5k Dataset (see README).\")\n",
    "        print(f\"Train images: {len(train_df)}, Test images: {len(test_df)}\")\n",
    "        print(f\"Categories: {train_df['dish_id'].nunique()}\")\n",
    "        print(\"Structure: imagery/metadata/splits.\")\n",
    "        print(\"Preprocessing: resizing to 224x224, normalization, outlier capping, augmentation.\")\n",
    "        print(\"Biases: Some classes have few samples; test set may contain unseen classes.\")\n",
    "        f.write(\"\\n\")\n",
    "        # 5. Methodology\n",
    "        f.write(\"5. Methodology\\n\")\n",
    "        f.write(\"==============\\n\")\n",
    "        f.write(\"5.1 Task Definition: Calorie regression.\\n\")\n",
    "        f.write(\"5.2 Model Selection: ResNet50 for feature extraction, small regression head.\\n\")\n",
    "        f.write(\"Training: Custom normalized metrics loss (MAE + RMSE + R2), Adam optimizer, regularization via dropout and batch norm.\\n\\n\")\n",
    "        print(\"5.1 Task Definition: Calorie regression.\")\n",
    "        print(\"5.2 Model Selection: ResNet50 for feature extraction, small regression head.\")\n",
    "        print(\"Training: Custom normalized metrics loss (MAE + RMSE + R2), Adam optimizer, regularization via dropout and batch norm.\")\n",
    "        f.write(\"\\n\")\n",
    "        # 6. Implementation Details\n",
    "        f.write(\"6. Implementation Details\\n\")\n",
    "        f.write(\"========================\\n\")\n",
    "        f.write(\"Frameworks: TensorFlow, Keras, scikit-learn, matplotlib, seaborn.\\n\")\n",
    "        f.write(\"Parameters: batch size 32, learning rate 1e-3, epochs up to 200, early stopping.\\n\")\n",
    "        f.write(\"Loss Function: Custom normalized metrics loss combining MAE, RMSE, and R2 across 5 nutrients.\\n\")\n",
    "        f.write(\"Hardware: [Specify GPU/CPU/Colab/local].\\n\")\n",
    "        f.write(\"Challenges: model overfitting, outlier handling, custom loss optimization.\\n\\n\")\n",
    "        print(\"Frameworks: TensorFlow, Keras, scikit-learn, matplotlib, seaborn.\")\n",
    "        print(\"Parameters: batch size 32, learning rate 1e-3, epochs up to 200, early stopping.\")\n",
    "        print(\"Loss Function: Custom normalized metrics loss combining MAE, RMSE, and R2 across 5 nutrients.\")\n",
    "        print(\"Hardware: [Specify GPU/CPU/Colab/local].\")\n",
    "        print(\"Challenges: model overfitting, outlier handling, custom loss optimization.\")\n",
    "        f.write(\"\\n\")\n",
    "        # 7. Results & Evaluation\n",
    "        f.write(\"7. Results & Evaluation\\n\")\n",
    "        f.write(\"=======================\\n\")\n",
    "        f.write(\"7.1 Calorie Estimation Metrics\\n\")\n",
    "        f.write(f\"  - MAE: {regression_metrics['mae']:.2f}\\n\")\n",
    "        print(f\"  - MAE: {regression_metrics['mae']:.2f}\")\n",
    "        f.write(f\"  - MSE: {regression_metrics['mse']:.2f}\\n\")\n",
    "        print(f\"  - MSE: {regression_metrics['mse']:.2f}\")\n",
    "        f.write(f\"  - RMSE: {regression_metrics['rmse']:.2f}\\n\")\n",
    "        print(f\"  - RMSE: {regression_metrics['rmse']:.2f}\")\n",
    "        f.write(f\"  - R²: {regression_metrics['r2']:.4f}\\n\")\n",
    "        print(f\"  - R²: {regression_metrics['r2']:.4f}\")\n",
    "        f.write(\"7.2 Comparison and Insights\\n\")\n",
    "        f.write(\"  - [Add comparison table and discussion here] \\n\\n\")\n",
    "        # 8. Innovation & Future Work\n",
    "        f.write(\"8. Innovation & Future Work\\n\")\n",
    "        f.write(\"===========================\\n\")\n",
    "        f.write(\"- Extensions: personalized feedback, portion size, mobile deployment.\\n\")\n",
    "        f.write(\"- Innovations: multi-task learning, ensembles, multimodal input.\\n\")\n",
    "        f.write(\"- Future: expand dataset, real-world deployment, user feedback.\\n\\n\")\n",
    "        print(\"- Extensions: personalized feedback, portion size, mobile deployment.\")\n",
    "        print(\"- Innovations: multi-task learning, ensembles, multimodal input.\")\n",
    "        print(\"- Future: expand dataset, real-world deployment, user feedback.\")\n",
    "        f.write(\"\\n\")\n",
    "        # 9. Conclusion\n",
    "        f.write(\"9. Conclusion\\n\")\n",
    "        f.write(\"==============\\n\")\n",
    "        f.write(\"This project demonstrates a robust, extensible pipeline for food image calorie estimation. The approach achieves strong regression results, with clear potential for real-world impact and future extension.\\n\\n\")\n",
    "        print(\"This project demonstrates a robust, extensible pipeline for food image calorie estimation. The approach achieves strong regression results, with clear potential for real-world impact and future extension.\")\n",
    "        f.write(\"\\n\")\n",
    "        # 10. References\n",
    "        f.write(\"10. References\\n\")\n",
    "        f.write(\"===============\\n\")\n",
    "        f.write(\"- Nutrition5k Dataset\\n- ResNet: He et al., 2015\\n- TensorFlow, Keras, scikit-learn\\n- [Add more as needed]\\n\\n\")\n",
    "        print(\"- Nutrition5k Dataset\")\n",
    "        print(\"- ResNet: He et al., 2015\")\n",
    "        print(\"- TensorFlow, Keras, scikit-learn\")\n",
    "        print(\"- [Add more as needed]\")\n",
    "        f.write(\"\\n\")\n",
    "        # 11. Appendix\n",
    "        f.write(\"11. Appendix\\n\")\n",
    "        f.write(\"==============\\n\")\n",
    "        f.write(\"- Model architecture diagrams: [Add if available]\\n\")\n",
    "        f.write(\"- Sample predictions: [Add screenshots if available]\\n\")\n",
    "        f.write(\"- Additional metrics: See CSVs and PNGs in results/\\n\")\n",
    "        print(\"- Model architecture diagrams: [Add if available]\")\n",
    "        print(\"- Sample predictions: [Add screenshots if available]\")\n",
    "        print(\"- Additional metrics: See CSVs and PNGs in results/\")\n",
    "        f.write(\"\\n\")\n",
    "    print(f\"Final report saved to {report_path}\")\n",
    "\n",
    "def save_detailed_metrics(baseline_metrics, improved_metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Save detailed metrics to CSV files for analysis.\n",
    "    \"\"\"\n",
    "    # Save overall metrics\n",
    "    overall_data = []\n",
    "    for metric in ['MAE', 'RMSE', 'R2']:\n",
    "        overall_data.append({\n",
    "            'Metric': metric,\n",
    "            'Baseline': baseline_metrics['Overall'][metric],\n",
    "            'Improved': improved_metrics['Overall'][metric],\n",
    "            'Improvement_Percent': ((improved_metrics['Overall'][metric] - baseline_metrics['Overall'][metric]) / \n",
    "                                  abs(baseline_metrics['Overall'][metric])) * 100\n",
    "        })\n",
    "    \n",
    "    overall_df = pd.DataFrame(overall_data)\n",
    "    overall_df.to_csv(os.path.join(output_dir, 'overall_metrics_detailed.csv'), index=False)\n",
    "    \n",
    "    # Save per-nutrient metrics\n",
    "    nutrient_data = []\n",
    "    nutrients = ['Calories', 'Mass', 'Fat', 'Carb', 'Protein']\n",
    "    for nutrient in nutrients:\n",
    "        for metric in ['MAE', 'RMSE', 'R2']:\n",
    "            nutrient_data.append({\n",
    "                'Nutrient': nutrient,\n",
    "                'Metric': metric,\n",
    "                'Baseline': baseline_metrics[nutrient][metric],\n",
    "                'Improved': improved_metrics[nutrient][metric],\n",
    "                'Improvement_Percent': ((improved_metrics[nutrient][metric] - baseline_metrics[nutrient][metric]) / \n",
    "                                      abs(baseline_metrics[nutrient][metric])) * 100\n",
    "            })\n",
    "    \n",
    "    nutrient_df = pd.DataFrame(nutrient_data)\n",
    "    nutrient_df.to_csv(os.path.join(output_dir, 'nutrient_metrics_detailed.csv'), index=False)\n",
    "    \n",
    "    print(f\"Detailed metrics saved to {output_dir}\")\n",
    "\n",
    "def plot_predictions_vs_actual(y_true, baseline_preds, improved_preds, output_dir):\n",
    "    \"\"\"\n",
    "    Generate scatter plots comparing predictions vs actual values for each nutrient.\n",
    "    \"\"\"\n",
    "    nutrients = ['Calories', 'Mass', 'Fat', 'Carb', 'Protein']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, nutrient in enumerate(nutrients):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot baseline predictions\n",
    "        ax.scatter(y_true[:, i], baseline_preds[:, i], alpha=0.6, label='Baseline', s=30)\n",
    "        # Plot improved predictions\n",
    "        ax.scatter(y_true[:, i], improved_preds[:, i], alpha=0.6, label='Improved', s=30)\n",
    "        \n",
    "        # Plot perfect prediction line\n",
    "        min_val = min(y_true[:, i].min(), baseline_preds[:, i].min(), improved_preds[:, i].min())\n",
    "        max_val = max(y_true[:, i].max(), baseline_preds[:, i].max(), improved_preds[:, i].max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8, label='Perfect Prediction')\n",
    "        \n",
    "        ax.set_xlabel(f'Actual {nutrient}')\n",
    "        ax.set_ylabel(f'Predicted {nutrient}')\n",
    "        ax.set_title(f'{nutrient} Predictions vs Actual')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove the last subplot (6th position)\n",
    "    fig.delaxes(axes[5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'predictions_vs_actual_all_nutrients.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def print_metrics_block(title, metrics):\n",
    "    print(f\"\\n-- {title.upper()} --\")\n",
    "    print(f\"MSE:    {metrics['MSE']:.4f}\")\n",
    "    print(f\"RMSE:   {metrics['RMSE']:.4f}\")\n",
    "    print(f\"MAE:    {metrics['MAE']:.4f}\")\n",
    "    print(f\"R²:     {metrics['R2']:.4f}\")\n",
    "    print(f\"MAPE:   {metrics['MAPE']:.2f}%\")\n",
    "    print(f\"NRMSE (range): {metrics['NRMSE']:.4f}\")\n",
    "    print(f\"NMAE (range):  {metrics['NMAE']:.4f}\")\n",
    "    print(f\"Norm factor:   {metrics['Norm factor']:.4f}\")\n",
    "\n",
    "def save_full_metrics_csv(baseline_metrics, improved_metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Save a single CSV with all metrics (MSE, RMSE, MAE, R2, MAPE, NRMSE, NMAE, Norm factor) for both models, for each nutrient and overall.\n",
    "    \"\"\"\n",
    "    metrics_list = ['MSE', 'RMSE', 'MAE', 'R2', 'MAPE', 'NRMSE', 'NMAE', 'Norm factor']\n",
    "    nutrients = ['Overall', 'Calories', 'Mass', 'Fat', 'Carb', 'Protein']\n",
    "    rows = []\n",
    "    for nutrient in nutrients:\n",
    "        row = {'Nutrient': nutrient}\n",
    "        for metric in metrics_list:\n",
    "            row[f'Baseline {metric}'] = baseline_metrics[nutrient][metric]\n",
    "            row[f'Improved {metric}'] = improved_metrics[nutrient][metric]\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(os.path.join(output_dir, 'full_metrics_comparison.csv'), index=False)\n",
    "    print(f\"Full metrics comparison saved to {os.path.join(output_dir, 'full_metrics_comparison.csv')}\")\n",
    "\n",
    "def main():\n",
    "    # Load full data (ignore split files)\n",
    "    full_df = load_full_data()\n",
    "    if full_df.empty:\n",
    "        print(\"Could not load data. Exiting.\")\n",
    "        return\n",
    "    # Filter to classes with at least 2 samples\n",
    "    class_counts = full_df['dish_id'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index\n",
    "    filtered_df = full_df[full_df['dish_id'].isin(valid_classes)].copy()\n",
    "    if len(filtered_df) == 0:\n",
    "        print(\"No classes with at least 2 samples. Using all available classes (no filtering).\")\n",
    "        filtered_df = full_df.copy()\n",
    "    else:\n",
    "        print(f\"After filtering: {len(filtered_df)} samples, {filtered_df['dish_id'].nunique()} classes with >=2 samples.\")\n",
    "    benchmark_dishes = ['dish_1565033189', 'dish_1556572657']\n",
    "    benchmark_rows = filtered_df[filtered_df['dish_id'].isin(benchmark_dishes)]\n",
    "    non_benchmark_df = filtered_df[~filtered_df['dish_id'].isin(benchmark_dishes)]\n",
    "    train_df, test_df = train_test_split(non_benchmark_df, test_size=0.2, random_state=42)\n",
    "    test_df = pd.concat([test_df, benchmark_rows]).drop_duplicates().reset_index(drop=True)\n",
    "    nutritional_cols = ['calories', 'mass', 'fat', 'carb', 'protein']\n",
    "    # Preprocess\n",
    "    train_df, caps = explore_and_process_data(train_df, 'train')\n",
    "    test_df, _ = explore_and_process_data(test_df, 'test', caps=caps)\n",
    "    # Feature extraction\n",
    "    base_model = build_regression_model()\n",
    "    train_features = extract_features(train_df, base_model)\n",
    "    test_features = extract_features(test_df, base_model)\n",
    "    # Scale labels\n",
    "    scaler = StandardScaler()\n",
    "    train_labels_scaled = scaler.fit_transform(train_df[nutritional_cols])\n",
    "    test_labels_scaled = scaler.transform(test_df[nutritional_cols])\n",
    "    # Baseline model\n",
    "    baseline_model, baseline_metrics, baseline_preds, baseline_y_true, baseline_history = run_regression_task(\n",
    "        train_features, train_labels_scaled, test_features, test_labels_scaled, scaler, model_type='baseline', epochs=10)\n",
    "    # Improved model\n",
    "    improved_model, improved_metrics, improved_preds, improved_y_true, improved_history = run_regression_task(\n",
    "        train_features, train_labels_scaled, test_features, test_labels_scaled, scaler, model_type='improved', epochs=200)\n",
    "    \n",
    "    # Plot training histories\n",
    "    plot_training_history(baseline_history, 'Baseline', OUTPUT_DIR)\n",
    "    plot_training_history(improved_history, 'Improved', OUTPUT_DIR)\n",
    "    \n",
    "    # Save models\n",
    "    baseline_model.save(os.path.join(OUTPUT_DIR, 'baseline_model.keras'))\n",
    "    improved_model.save(os.path.join(OUTPUT_DIR, 'improved_model.keras'))\n",
    "    \n",
    "    # Save detailed metrics\n",
    "    save_detailed_metrics(baseline_metrics, improved_metrics, OUTPUT_DIR)\n",
    "    \n",
    "    # Generate predictions vs actual plots\n",
    "    plot_predictions_vs_actual(baseline_y_true, baseline_preds, improved_preds, OUTPUT_DIR)\n",
    "    \n",
    "    # Table 1: Overall metrics and improvement\n",
    "    print(\"\\nTable 1: Overall Model Performance\")\n",
    "    table1 = [\n",
    "        [\"MAE\", baseline_metrics['Overall']['MAE'], improved_metrics['Overall']['MAE'],\n",
    "         f\"{100*(improved_metrics['Overall']['MAE']-baseline_metrics['Overall']['MAE'])/abs(baseline_metrics['Overall']['MAE']):.1f}%\"],\n",
    "        [\"RMSE\", baseline_metrics['Overall']['RMSE'], improved_metrics['Overall']['RMSE'],\n",
    "         f\"{100*(improved_metrics['Overall']['RMSE']-baseline_metrics['Overall']['RMSE'])/abs(baseline_metrics['Overall']['RMSE']):.1f}%\"],\n",
    "        [\"R2\", baseline_metrics['Overall']['R2'], improved_metrics['Overall']['R2'],\n",
    "         f\"{100*(improved_metrics['Overall']['R2']-baseline_metrics['Overall']['R2'])/abs(baseline_metrics['Overall']['R2']):.1f}%\"]\n",
    "    ]\n",
    "    print(tabulate(table1, headers=[\"Metric\", \"Baseline Model\", \"Fine-tuned Model\", \"Improvement\"], floatfmt=\".4f\"))\n",
    "    pd.DataFrame(table1, columns=[\"Metric\", \"Baseline Model\", \"Fine-tuned Model\", \"Improvement\"]).to_csv(os.path.join(OUTPUT_DIR, 'table1_overall_metrics.csv'), index=False)\n",
    "    # Table 2: Per-nutrient metrics\n",
    "    print(\"\\nTable 2: Per-Nutrient Performance (Test Set)\")\n",
    "    table2 = []\n",
    "    for nutrient in ['Calories', 'Mass', 'Fat', 'Carb', 'Protein']:\n",
    "        table2.append([\n",
    "            nutrient,\n",
    "            baseline_metrics[nutrient]['MAE'], improved_metrics[nutrient]['MAE'],\n",
    "            baseline_metrics[nutrient]['RMSE'], improved_metrics[nutrient]['RMSE'],\n",
    "            baseline_metrics[nutrient]['R2'], improved_metrics[nutrient]['R2']\n",
    "        ])\n",
    "    print(tabulate(table2, headers=[\"Nutrient\", \"Baseline MAE\", \"Improved MAE\", \"Baseline RMSE\", \"Improved RMSE\", \"Baseline R2\", \"Improved R2\"], floatfmt=\".4f\"))\n",
    "    pd.DataFrame(table2, columns=[\"Nutrient\", \"Baseline MAE\", \"Improved MAE\", \"Baseline RMSE\", \"Improved RMSE\", \"Baseline R2\", \"Improved R2\"]).to_csv(os.path.join(OUTPUT_DIR, 'table2_per_nutrient_metrics.csv'), index=False)\n",
    "    # Table 3: Benchmark dish predictions (improved readable format)\n",
    "    print(\"\\nTable 3: Benchmark Dish Predictions (Actual vs Predicted)\")\n",
    "    for dish_id in benchmark_dishes:\n",
    "        dish_row = test_df[test_df['dish_id'] == dish_id]\n",
    "        if dish_row.empty:\n",
    "            print(f\"Benchmark dish {dish_id} not found in test set.\")\n",
    "            continue\n",
    "        dish_features = extract_features(dish_row, base_model)\n",
    "        dish_labels = dish_row[nutritional_cols].values\n",
    "        pred_baseline = scaler.inverse_transform(baseline_model.predict(dish_features))\n",
    "        pred_improved = scaler.inverse_transform(improved_model.predict(dish_features))\n",
    "        print(f\"\\nDish: {dish_id}\")\n",
    "        print(f\"{'Nutrient':<10} {'Actual':>10} {'Baseline Pred':>18} {'Improved Pred':>18}\")\n",
    "        for i, nutrient in enumerate(nutritional_cols):\n",
    "            print(f\"{nutrient.capitalize():<10} {dish_labels[0, i]:>10.2f} {pred_baseline[0, i]:>18.2f} {pred_improved[0, i]:>18.2f}\")\n",
    "    # Print metrics in screenshot format for improved model\n",
    "    print(\"\\n========================\\nDETAILED METRICS (IMPROVED MODEL)\\n========================\")\n",
    "    print_metrics_block('OVERALL', improved_metrics['Overall'])\n",
    "    for nutrient in ['Calories', 'Mass', 'Fat', 'Carb', 'Protein']:\n",
    "        print_metrics_block(nutrient, improved_metrics[nutrient])\n",
    "    # Print metrics in screenshot format for baseline model\n",
    "    print(\"\\n========================\\nDETAILED METRICS (BASELINE MODEL)\\n========================\")\n",
    "    print_metrics_block('OVERALL', baseline_metrics['Overall'])\n",
    "    for nutrient in ['Calories', 'Mass', 'Fat', 'Carb', 'Protein']:\n",
    "        print_metrics_block(nutrient, baseline_metrics[nutrient])\n",
    "    # Save full metrics CSV with all metrics for both models\n",
    "    save_full_metrics_csv(baseline_metrics, improved_metrics, OUTPUT_DIR)\n",
    "    print(\"\\n--- Pipeline Finished ---\")\n",
    "    print(f\"All models, reports, and visualizations saved in '{OUTPUT_DIR}' directory.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
